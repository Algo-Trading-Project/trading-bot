{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering and model selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models and metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Analytical helper functions to declutter the notebook\n",
    "from analysis.ml.feature_engineering.custom_transformers import *\n",
    "from analysis.ml.data import calculate_label_uniquness\n",
    "from analysis.stats import *\n",
    "from analysis.ml.data import get_ml_dataset, get_ml_features\n",
    "\n",
    "# Other imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify window sizes for rolling min-max and z-score scaling\n",
    "window_sizes_scaling = [7, 30, 90, 180, 365]\n",
    "\n",
    "# Specify window sizes for returns-based features\n",
    "window_sizes_returns = [1, 7, 30, 90, 180, 365]\n",
    "\n",
    "# Specify lookback window for correlation features\n",
    "lookback_windows_correlation = [7, 30, 90, 180, 365]\n",
    "\n",
    "# Specify maximum holding times for triple barrier labeling\n",
    "max_holding_times_triple_barrier_label = [7]\n",
    "\n",
    "# Specify lookback window for horizontal barriers for triple barrier labeling\n",
    "std_lookback_windows = [7]\n",
    "\n",
    "# Pipeline for feature engineering and modeling\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "\n",
    "    ('ta_features', TAFeatures(windows = window_sizes_scaling)),\n",
    "\n",
    "    ('time_features', TimeFeatures()),\n",
    "\n",
    "    # ('returns_features', ReturnsFeatures(\n",
    "    #     window_sizes = window_sizes_returns,\n",
    "    #     lookback_windows = lookback_windows_correlation,\n",
    "    # )),\n",
    "\n",
    "    ('order_book_features', OrderBookFeatures()),\n",
    "\n",
    "    ('correlation_features', CorrelationFeatures(\n",
    "        window_sizes = window_sizes_returns, \n",
    "        lookback_windows = lookback_windows_correlation,\n",
    "        period = '1d'\n",
    "    )),\n",
    "\n",
    "    ('rolling_z_score', RollingZScoreScaler(window_sizes = window_sizes_scaling)),\n",
    "\n",
    "    ('fill_na', FillNaTransformer()),\n",
    "\n",
    "    # ('lag_features', LagFeatures([1, 2, 3, 4, 5, 6, 7])),\n",
    "\n",
    "    ('triple_barrier_labels', TripleBarrierLabelFeatures(\n",
    "        max_holding_times = max_holding_times_triple_barrier_label, \n",
    "        std_lookback_windows = std_lookback_windows\n",
    "    )),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_ml_features()\n",
    "print('X shape:', X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we need to drop before training the model\n",
    "triple_barrier_label_cols = [\n",
    "    col for col in X if 'triple_barrier_label_h' in col\n",
    "]\n",
    "\n",
    "trade_returns_cols = [\n",
    "    col for col in X if 'trade_returns' in col\n",
    "]\n",
    "\n",
    "non_numeric_cols = [\n",
    "    'asset_id_base', 'asset_id_quote', 'exchange_id', 'Unnamed: 0'\n",
    "]\n",
    "\n",
    "forward_returns_cols = [\n",
    "    'open', 'high', 'low', 'close', 'start_date_triple_barrier_label_h24', \n",
    "    'end_date_triple_barrier_label_h24', 'avg_uniqueness', 'time_period_end'\n",
    "]\n",
    "\n",
    "cols_to_drop = (\n",
    "    triple_barrier_label_cols + \n",
    "    trade_returns_cols + \n",
    "    non_numeric_cols +\n",
    "    forward_returns_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure train_date_cutoff is a datetime object to prevent data leakage\n",
    "train_date_cutoff = X['time_period_end'].quantile(0.7)\n",
    "\n",
    "# Split the dataset into training and testing set\n",
    "X_train = X[X['time_period_end'] <= train_date_cutoff]\n",
    "X_test = X[\n",
    "    (X['time_period_end'] > train_date_cutoff) &\n",
    "    (X['time_period_end'] <= '2023-12-31')\n",
    "]\n",
    "\n",
    "train_min_date = X_train['time_period_end'].min()\n",
    "train_max_date = X_train['time_period_end'].max()\n",
    "\n",
    "test_min_date = X_test['time_period_end'].min()\n",
    "test_max_date = X_test['time_period_end'].max()\n",
    "\n",
    "print('Training set date range:', train_min_date, train_max_date)\n",
    "print('Testing set date range:', test_min_date, test_max_date)\n",
    "\n",
    "# Ensure no data leakage\n",
    "data_leakage_indicator = (\n",
    "    # Ensure that the training set doesn't overlap with the testing set\n",
    "    (X_train['time_period_end'].max() >= X_test['time_period_end'].min())\n",
    ")\n",
    "assert not data_leakage_indicator, 'Data leakage detected!'\n",
    "\n",
    "y_train = ((X_train['triple_barrier_label_h7'] == 1) | ((X_train['triple_barrier_label_h7'] == 0) & (X_train['trade_returns_h7'] > 0))).astype(int)\n",
    "y_test = ((X_test['triple_barrier_label_h7'] == 1) | ((X_test['triple_barrier_label_h7'] == 0) & (X_test['trade_returns_h7'] > 0))).astype(int)\n",
    "\n",
    "y_train_reg = X_train['returns_1'].shift(-1).abs()\n",
    "y_test_reg = X_test['returns_1'].shift(-1).abs()\n",
    "\n",
    "# Filters for the training and testing set\n",
    "# train_filter = (\n",
    "#     X_train['returns_1_rz_24'].abs() >= 1\n",
    "# )\n",
    "\n",
    "# test_filter = (\n",
    "#     X_test['returns_1_rz_24'] >= 1\n",
    "# )\n",
    "\n",
    "# Apply the filters (Optional)\n",
    "# X_train = X_train[train_filter]\n",
    "# X_test = X_test[test_filter]\n",
    "\n",
    "# y_train = y_train[train_filter]\n",
    "# y_test = y_test[test_filter]\n",
    "\n",
    "# Make these columns categorical for XGBoost\n",
    "X_train[['symbol_id', 'time_period_end']] = X_train[['symbol_id', 'time_period_end']].astype('category')\n",
    "X_test[['symbol_id', 'time_period_end']] = X_test[['symbol_id', 'time_period_end']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print()\n",
    "print('Y train distribution:')\n",
    "print(y_train.value_counts(normalize = True))\n",
    "print()\n",
    "print('Y test distribution:')\n",
    "print(y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "path = '/Users/louisspencer/Desktop/Trading-Bot/data/pretrained_models/classification/xgboost_model_2023_july_to_sept.pkl'\n",
    "model = joblib.load(path)\n",
    "model.enable_categorical = True\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curve for calibrated model on test set\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 6))\n",
    "\n",
    "brier_calibrated = brier_score_loss(y_test, model.predict_proba(X_test.drop(columns = ['time_period_end'], errors = 'ignore'))[:, 1])\n",
    "display = CalibrationDisplay.from_estimator(model, X_test.drop(columns = ['time_period_end'], errors = 'ignore'), y_test, ax = ax)\n",
    "display.plot(ax = ax)\n",
    "ax.set_title(f'Calibration Curve (Calibrated Model, Brier Score: {brier_calibrated:.4f})')\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "pred_prob_test = model.predict_proba(X_test.drop(columns = cols_to_drop + ['time_period_end'], errors = 'ignore'))[:, 1]\n",
    "y_pred_test = model.predict(X_test.drop(columns = cols_to_drop + ['time_period_end'], errors = 'ignore'))\n",
    "\n",
    "print('XGBoost Test Performance:')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print()\n",
    "print('Baseline Model Performance (Random):')\n",
    "print()\n",
    "print(classification_report(y_test, np.random.randint(0, 2, len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve (2 subplots)\n",
    "fig, ax = plt.subplots(1, 2, figsize = (18, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred_prob_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "sns.lineplot(x = fpr, y = tpr, ax = ax[0])\n",
    "sns.lineplot(x = [0, 1], y = [0, 1], color = 'black', linestyle = '--', ax = ax[0])\n",
    "ax[0].set_title(f'ROC Curve (AUC: {roc_auc:.3f})')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, pred_prob_test)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "sns.lineplot(x = recall, y = precision, ax = ax[1])\n",
    "ax[1].set_title(f'Precision-Recall Curve (PR_AUC: {pr_auc:.3f})')\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precision')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top N most important features as horizontal bar plot\n",
    "top_n = 40\n",
    "feature_importances = pd.Series(model.feature_importances_, index = X_test.columns)\n",
    "feature_importances = feature_importances.sort_values().tail(top_n)\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "feature_importances.plot(kind = 'barh')\n",
    "plt.title(f'Top {top_n} Most Important Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sample statistic on the test set\n",
    "X_test['y_pred'] = y_pred_test\n",
    "sample_means = X_test[['y_pred', 'trade_returns_h7']].groupby('y_pred').mean()\n",
    "sample_statistic = round(sample_means.loc[1, 'trade_returns_h7'] - sample_means.loc[0, 'trade_returns_h7'], 3)\n",
    "\n",
    "# Calculate the information coefficient of the model's predictions\n",
    "information_coefficient = round(X_test[['y_pred', 'trade_returns_h7']].corr().iloc[0, 1], 3)\n",
    "print('Information Coefficient:', information_coefficient)\n",
    "\n",
    "# Calculate the sample statistic for a random baseline\n",
    "X_test['y_pred_random'] = np.random.randint(0, 2, len(X_test))\n",
    "sample_means_random = X_test[['y_pred_random', 'trade_returns_h7']].groupby('y_pred_random').mean()\n",
    "sample_statistic_random = round(sample_means_random.loc[1, 'trade_returns_h7'] - sample_means_random.loc[0, 'trade_returns_h7'], 3)\n",
    "\n",
    "print(f'Sample statistic: {sample_statistic}')\n",
    "print(f'Random baseline statistic: {sample_statistic_random}')\n",
    "print()\n",
    "print(sample_means)\n",
    "print()\n",
    "print(sample_means_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average trade returns for true positives (correct 1 predictions)\n",
    "mean_true_positive_returns = X_test[(X_test['y_pred'] == 1) & (y_test == 1)]['trade_returns_h7'].mean()\n",
    "# Average trade returns for false positives (incorrect 1 predictions)\n",
    "mean_false_positive_returns = X_test[(X_test['y_pred'] == 1) & (y_test == 0)]['trade_returns_h7'].mean()\n",
    "# Average trade returns for true negatives (correct 0 predictions)\n",
    "mean_true_negative_returns = X_test[(X_test['y_pred'] == 0) & (y_test == 0)]['trade_returns_h7'].mean()\n",
    "# Average trade returns for false negatives (incorrect 0 predictions)\n",
    "mean_false_negative_returns = X_test[(X_test['y_pred'] == 0) & (y_test == 1)]['trade_returns_h7'].mean()\n",
    "\n",
    "tp_minus_fp = round(mean_true_positive_returns - abs(mean_false_positive_returns), 3)\n",
    "\n",
    "print('Mean Trade Returns for True Positives:', mean_true_positive_returns) \n",
    "print('Mean Trade Returns for False Positives:', mean_false_positive_returns)\n",
    "print('Mean Trade Returns for True Negatives:', mean_true_negative_returns)\n",
    "print('Mean Trade Returns for False Negatives:', mean_false_negative_returns)\n",
    "print()\n",
    "print('(TP - FP):', tp_minus_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Expectancy\n",
    "win_rate = (y_pred_test == y_test).mean()\n",
    "avg_win = mean_true_positive_returns\n",
    "avg_loss = abs(mean_false_positive_returns)\n",
    "\n",
    "expectancy = (win_rate * avg_win) - ((1 - win_rate) * avg_loss)\n",
    "print('Expectancy:', round(expectancy, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_avg_pred_returns = X_test.groupby(['symbol_id', 'y_pred'])['trade_returns_h7'].mean()\n",
    "positive = oos_avg_pred_returns.loc[(slice(None), 1)]\n",
    "positive = positive.reindex(X_test['symbol_id'].unique(), fill_value = 0)\n",
    "\n",
    "negative = oos_avg_pred_returns.loc[(slice(None), 0)]\n",
    "negative = negative.reindex(X_test['symbol_id'].unique(), fill_value = 0)\n",
    "\n",
    "positive = positive.sort_values()\n",
    "negative = negative.sort_values()\n",
    "\n",
    "diff = pd.DataFrame({\n",
    "    'positive': positive,\n",
    "    'negative': negative\n",
    "})\n",
    "diff['diff'] = diff['positive'] - diff['negative']\n",
    "diff = diff.sort_values('diff', ascending = False)\n",
    "\n",
    "# Bar plot of the average positive prediction returns across different symbols (Seaborn)\n",
    "plt.figure(figsize = (30, 7))\n",
    "sns.barplot(x = positive.index.get_level_values(0), y = positive.values)\n",
    "plt.title('Average Positive Prediction Returns Across Different Symbols')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of the average negative prediction returns across different symbols (Seaborn)\n",
    "plt.figure(figsize = (30, 7))\n",
    "sns.barplot(x = negative.index.get_level_values(0), y = negative.values)\n",
    "plt.title('Average Negative Prediction Returns Across Different Symbols')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of the difference in average returns across different symbols (Seaborn)\n",
    "plt.figure(figsize = (30, 7))\n",
    "sns.barplot(x = diff.index.get_level_values(0), y = diff['diff'])\n",
    "plt.title('Difference in Average Returns Across Different Symbols')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the OOS statistical tests to check model robustness\n",
    "run_oos_statistical_tests(\n",
    "    y_pred_test, \n",
    "    X_test['trade_returns_h7'].values, \n",
    "    n_simulations = 1_000_000,\n",
    "    sample_statistic = sample_statistic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the y_pred column\n",
    "drop_cols = ['y_pred', 'forward_returns_7','forward_returns_30', 'forward_returns_1', 'forward_avg_returns_1_7d', 'forward_pos_neg_volatility_ratio_7', 'forward_returns_7']\n",
    "X_test.drop(columns = drop_cols, errors = 'ignore', inplace = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
