{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering and model selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Custom imports to prevent clutter\n",
    "from helper import *\n",
    "from custom_transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_ml_dataset():\n",
    "    # Construct the dataset for machine learning\n",
    "    if not os.path.exists('/Users/louisspencer/Desktop/Trading-Bot/data/ml_dataset.csv'):\n",
    "        df = construct_dataset_for_ml()\n",
    "\n",
    "        print('dataset shape:', df.shape)\n",
    "        print(f'database size before downcasting: {df.memory_usage(deep = True).sum() / 1e9} GB')\n",
    "\n",
    "        numeric_cols = [col for col in df.columns if col not in ('symbol_id', 'time_period_end')]\n",
    "        categorical_cols = ['symbol_id']\n",
    "\n",
    "        df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, downcast = 'float')\n",
    "        df[categorical_cols] = df[categorical_cols].astype('category')\n",
    "\n",
    "        print(f'database size after downcasting: {df.memory_usage(deep = True).sum() / 1e9} GB')\n",
    "    else:\n",
    "        df = pd.read_csv('/Users/louisspencer/Desktop/Trading-Bot/data/ml_dataset.csv', index_col = 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_ml_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_columns(X):\n",
    "    # Remove one_hot__ and remainder__ from column names\n",
    "    X.columns = X.columns.str.replace('one_hot__', '')\n",
    "    X.columns = X.columns.str.replace('remainder__', '')\n",
    "    X.columns = X.columns.str.replace('symbol_id_', '')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify window sizes for rolling min-max and z-score scaling\n",
    "window_sizes_scaling = [2 * 12, 2 * 24, 2 * 24 * 7, 2 * 24 * 30]\n",
    "\n",
    "# Specify window sizes for returns-based features\n",
    "window_sizes_returns = [1, 2 * 24, 2 * 24 * 7, 2 * 24 * 30]\n",
    "\n",
    "# Specify the window size for regression-based features\n",
    "window_sizes_regression = [2 * 24, 2 * 24 * 7, 2 * 24 * 30]\n",
    "\n",
    "windows_triple_barrier_label = [2 * 12, 2 * 24, 2 * 24 * 7, 2 * 24 * 30]\n",
    "max_holding_times_triple_barrier_label = [2 * 12, 2 * 24, 2 * 24 * 7, 2 * 24 * 30]\n",
    "\n",
    "# Pipeline for feature engineering and modeling\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "\n",
    "    # Add regression-based features to the dataset\n",
    "    ('regression_features', RegressionFeatures(window_sizes_regression)),\n",
    "    \n",
    "    # Add returns-based features to the dataset\n",
    "    ('returns_features', ReturnsFeatures(window_sizes_returns)),\n",
    "\n",
    "    # Add correlation-based features to the dataset\n",
    "    ('correlation_features', CorrelationFeatures(window_sizes_returns)),\n",
    "\n",
    "    # Add triple-barrier label features to the dataset\n",
    "    ('triple_barrier_label_features', TripleBarrierLabelFeatures(windows_triple_barrier_label, max_holding_times_triple_barrier_label)),\n",
    "\n",
    "    # Add price-based features to the dataset\n",
    "    ('price_features', PriceFeatures()),\n",
    "\n",
    "    # Add rolling z-score scaled features to the dataset\n",
    "    ('rolling_z_score_scaler', RollingZScoreScaler(window_sizes_scaling)),\n",
    "\n",
    "    # Clean NaN/infinity values from the dataset\n",
    "    ('fill_nan', FillNaN()),\n",
    "\n",
    "])\n",
    "\n",
    "data_cleaning_pipeline = Pipeline([\n",
    "    \n",
    "    # One-hot encode the symbol_id column\n",
    "    ('one_hot_encoding', ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('one_hot', OneHotEncoder(sparse_output=False), ['symbol_id'])\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    # Clean the column names\n",
    "    ('clean_column_names', FunctionTransformer(clean_columns))\n",
    "    \n",
    "]).set_output(transform = 'pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing symbol_id: AAVE_USDT_KUCOIN (1/67)\n",
      "Processing symbol_id: AGIX_USDT_OKEX (2/67)\n",
      "Processing symbol_id: ALGO_USDT_BINANCE (3/67)\n",
      "Processing symbol_id: ALGO_USD_COINBASE (4/67)\n",
      "Processing symbol_id: ARKM_USDT_BINANCE (5/67)\n",
      "Processing symbol_id: ATOM_USDT_OKEX (6/67)\n",
      "Processing symbol_id: BCH_USDT_OKEX (7/67)\n",
      "Processing symbol_id: BCH_USD_COINBASE (8/67)\n",
      "Processing symbol_id: BNB_USDT_BINANCE (9/67)\n",
      "Processing symbol_id: BNB_USDT_OKEX (10/67)\n",
      "Processing symbol_id: BTC_USD_COINBASE (11/67)\n",
      "Processing symbol_id: BTC_USD_BITSTAMP (12/67)\n",
      "Processing symbol_id: BTT_USDT_BINANCE (13/67)\n",
      "Processing symbol_id: CRV_USDT_OKEX (14/67)\n",
      "Processing symbol_id: DAI_USD_COINBASE (15/67)\n",
      "Processing symbol_id: DOGE_USDT_BINANCE (16/67)\n",
      "Processing symbol_id: DOT_USDT_KUCOIN (17/67)\n"
     ]
    }
   ],
   "source": [
    "def get_ml_features():\n",
    "    if not os.path.exists('/Users/louisspencer/Desktop/Trading-Bot/data/ml_features.csv'):\n",
    "        X = []\n",
    "        i = 1\n",
    "        n = len(df.symbol_id.unique())\n",
    "\n",
    "        for symbol_id in df.symbol_id.unique():                 \n",
    "            print(f'Processing symbol_id: {symbol_id} ({i}/{n})')\n",
    "            i += 1\n",
    "\n",
    "            token = QUERY(\n",
    "                f\"\"\"\n",
    "                SELECT *\n",
    "                FROM market_data.ml_dataset\n",
    "                WHERE symbol_id = '{symbol_id}'\n",
    "                ORDER BY time_period_end\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            features = feature_engineering_pipeline.fit_transform(token)\n",
    "            X.append(features)\n",
    "\n",
    "        X = pd.concat(X)\n",
    "        X.to_csv('/Users/louisspencer/Desktop/Trading-Bot/data/ml_features.csv', index = True)\n",
    "    else:\n",
    "        # X = QUERY(\n",
    "        #     \"\"\"\n",
    "        #     SELECT *\n",
    "        #     FROM market_data.ml_features\n",
    "        #     ORDER BY symbol_id, time_period_end\n",
    "        #     \"\"\"\n",
    "        # ).set_index('time_period_end')\n",
    "\n",
    "        X = pd.read_csv('/Users/louisspencer/Desktop/Trading-Bot/data/ml_features.csv', index_col = 0)\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "    \n",
    "    return X\n",
    "\n",
    "X = get_ml_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_barrier_label_cols = [\n",
    "    f'triple_barrier_label_w{w}_h{h}' \n",
    "    for w in windows_triple_barrier_label \n",
    "    for h in max_holding_times_triple_barrier_label\n",
    "]\n",
    "\n",
    "trade_returns_cols = [\n",
    "    f'trade_returns_w{w}_h{h}' \n",
    "    for w in windows_triple_barrier_label \n",
    "    for h in max_holding_times_triple_barrier_label\n",
    "]\n",
    "\n",
    "non_z_score_cols = [c for c in X.columns if '_rz_' not in c]\n",
    "\n",
    "cols_to_drop = triple_barrier_label_cols + trade_returns_cols + non_z_score_cols + ['symbol_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[X.index <= '2023-06-01']\n",
    "X_test = X[X.index > '2023-06-01']\n",
    "\n",
    "X_train = X_train.sample(n = 1_000_000, replace = False)\n",
    "X_test = X_test.sample(n = 500_000, replace = False)\n",
    "\n",
    "y_train = X_train['triple_barrier_label_w336_h336']\n",
    "y_test = X_test['triple_barrier_label_w336_h336']\n",
    "\n",
    "X_train = X_train.drop(columns = cols_to_drop)\n",
    "X_test = X_test.drop(columns = cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train size before downcasting: {X_train.memory_usage(deep = True).sum() / 1e9} GB')\n",
    "print(f'X_test size before downcasting: {X_test.memory_usage(deep = True).sum() / 1e9} GB')\n",
    "print()\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in X_train.columns if c not in ('symbol_id', 'time_period_end')]\n",
    "\n",
    "# Downcast data to save memory\n",
    "X_train[numeric_cols] = X_train[numeric_cols].apply(pd.to_numeric, downcast = 'float')\n",
    "X_test[numeric_cols] = X_test[numeric_cols].apply(pd.to_numeric, downcast = 'float')\n",
    "\n",
    "print(f'X_train size after downcasting: {X_train.memory_usage(deep = True).sum() / 1e9} GB')\n",
    "print(f'X_test size after downcasting: {X_test.memory_usage(deep = True).sum() / 1e9} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# One-hot encode the symbol_id column\n",
    "# X_train = data_cleaning_pipeline.fit_transform(X_train)\n",
    "# X_test = data_cleaning_pipeline.fit_transform(X_test)\n",
    "\n",
    "# print(f'X_train size: {X_train.memory_usage(deep = True).sum() / 1e9} GB')\n",
    "# print(f'X_test size: {X_test.memory_usage(deep = True).sum() / 1e9} GB')\n",
    "\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Y train distribution:')\n",
    "print(y_train.value_counts(normalize = True))\n",
    "print()\n",
    "print('Y test distribution:')\n",
    "print(y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    max_depth = 10,\n",
    "    bootstrap = True,\n",
    "    random_state = 9 + 10,\n",
    "    n_jobs = -1,\n",
    "    verbose = True,\n",
    "    class_weight = 'balanced'\n",
    ")\n",
    "\n",
    "rf.fit(\n",
    "    X_train,\n",
    "    y_train, \n",
    "    sample_weight = X_train['returns_1'].abs().values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.verbose = False\n",
    "\n",
    "# Predictions on the training and test set\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_pred_test_rf = rf.predict(X_test)\n",
    "\n",
    "# Predicted probabilities on the training and test set for each class\n",
    "y_pred_proba_train_rf = rf.predict_proba(X_train)\n",
    "y_pred_proba_test_rf = rf.predict_proba(X_test)\n",
    "\n",
    "# Classification reports for the training and test set\n",
    "print('RF Train:')\n",
    "print(classification_report(y_train, y_pred_train_rf))\n",
    "print()\n",
    "print('RF Test:')\n",
    "print(classification_report(y_test, y_pred_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "print('Baseline model:')\n",
    "print('Randomly predicting a class:')\n",
    "print()\n",
    "\n",
    "# Classification reports for the training and test set\n",
    "print(classification_report(y_test, np.random.choice([-1, 0, 1], size = len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top N most important features as horizontal bar plot\n",
    "top_n = 50\n",
    "feature_importances = pd.Series(rf.feature_importances_, index = X_train.columns)\n",
    "feature_importances = feature_importances.sort_values().tail(top_n)\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "feature_importances.plot(kind = 'barh')\n",
    "plt.title(f'Top {top_n} Most Important Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training the meta model\n",
    "\n",
    "# Create Meta Labels\n",
    "meta_labels_train = (y_pred_train_rf == y_train).astype(int)\n",
    "meta_labels_test = (y_pred_test_rf == y_test).astype(int)\n",
    "\n",
    "# Copy the original dataset for meta-labeling\n",
    "X_train_meta = X_train.copy()\n",
    "X_test_meta = X_test.copy()\n",
    "\n",
    "# Add the predictions of the Random Forest model as features for the meta model\n",
    "X_train_meta['rf_predictions'] = y_pred_train_rf\n",
    "X_test_meta['rf_predictions'] = y_pred_test_rf\n",
    "\n",
    "# Use the predicted probabilities for each class as features for the meta model\n",
    "class_0_index = rf.classes_.tolist().index(0)\n",
    "class_1_index = rf.classes_.tolist().index(1)\n",
    "class_neg_1_index = rf.classes_.tolist().index(-1)\n",
    "\n",
    "X_train_meta['rf_proba_0'] = y_pred_proba_train_rf[:, class_0_index]\n",
    "X_train_meta['rf_proba_1'] = y_pred_proba_train_rf[:, class_1_index]\n",
    "X_train_meta['rf_proba_-1'] = y_pred_proba_train_rf[:, class_neg_1_index]\n",
    "\n",
    "X_test_meta['rf_proba_0'] = y_pred_proba_test_rf[:, class_0_index]\n",
    "X_test_meta['rf_proba_1'] = y_pred_proba_test_rf[:, class_1_index]\n",
    "X_test_meta['rf_proba_-1'] = y_pred_proba_test_rf[:, class_neg_1_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-labeling with Random Forest\n",
    "\n",
    "# Train a new Random Forest model on the meta labels\n",
    "rf_meta = RandomForestClassifier(\n",
    "    bootstrap = True,\n",
    "    random_state = 9 + 10,\n",
    "    n_jobs = -1,\n",
    "    verbose = True,\n",
    "    class_weight = 'balanced'\n",
    ")\n",
    "\n",
    "# Fit the Random Forest meta model\n",
    "rf_meta.fit(X_train_meta, meta_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-labeling with Neural Network\n",
    "\n",
    "# Train a new Neural Network model on the meta labels\n",
    "nn_meta = MLPClassifier(\n",
    "    hidden_layer_sizes = (500, 250, 125),\n",
    "    learning_rate = 'adaptive',\n",
    "    verbose = True,\n",
    "    activation = 'relu',\n",
    "    solver = 'adam',\n",
    "    random_state = 9 + 10\n",
    ")\n",
    "\n",
    "# Fit the Neural Network meta model\n",
    "nn_meta.fit(X_train_meta, meta_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distrubution of meta labels\n",
    "print('Meta labels train distribution:')\n",
    "print(meta_labels_train.value_counts(normalize = True))\n",
    "print()\n",
    "\n",
    "print('Meta labels test distribution:')\n",
    "print(meta_labels_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Meta Model Performance\n",
    "\n",
    "rf_meta.verbose = False\n",
    "\n",
    "# Predictions on the training and test set\n",
    "y_pred_train_rf_meta = rf_meta.predict(X_train_meta)\n",
    "y_pred_test_rf_meta = rf_meta.predict(X_test_meta)\n",
    "\n",
    "# Classification reports for the training and test set\n",
    "print('RF Meta Train:')\n",
    "print(classification_report(meta_labels_train, y_pred_train_rf_meta))\n",
    "print()\n",
    "print('RF Meta Test:')\n",
    "print(classification_report(meta_labels_test, y_pred_test_rf_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Meta Model Performance\n",
    "nn_meta.verbose = False\n",
    "\n",
    "# Predictions on the training and test set\n",
    "y_pred_train_nn_meta = nn_meta.predict(X_train_meta)\n",
    "y_pred_test_nn_meta = nn_meta.predict(X_test_meta)\n",
    "\n",
    "# Classification reports for the training and test set\n",
    "print('NN Meta Train:')\n",
    "print(classification_report(meta_labels_train, y_pred_train_nn_meta))\n",
    "print()\n",
    "\n",
    "print('NN Meta Test:')\n",
    "print(classification_report(meta_labels_test, y_pred_test_nn_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top N most important features as horizontal bar plot\n",
    "top_n = 40\n",
    "feature_importances = pd.Series(rf_meta.feature_importances_, index = X_train_meta.columns)\n",
    "feature_importances = feature_importances.sort_values().tail(top_n)\n",
    "\n",
    "plt.figure(figsize = (10, 7))\n",
    "feature_importances.plot(kind = 'barh')\n",
    "plt.title(f'Top {top_n} Most Important Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how much the meta model improves the base model when applied to the test set\n",
    "print('Base model:')\n",
    "print(classification_report(y_test, y_pred_test_rf))\n",
    "print()\n",
    "\n",
    "# Filter out the trades where the base model was wrong according to the meta model\n",
    "X_test_filtered = X_test[y_pred_test_rf_meta == 1]\n",
    "y_test_filtered = y_test[y_pred_test_rf_meta == 1]\n",
    "\n",
    "# Predictions on the filtered test set\n",
    "y_pred_test_rf_filtered = rf.predict(X_test_filtered)\n",
    "\n",
    "# Classification report for the filtered test set\n",
    "print('Base model on filtered test set:')\n",
    "print(classification_report(y_test_filtered, y_pred_test_rf_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performance():\n",
    "    # Train and test a Random Forest model on each individual symbol\n",
    "    performance_dict = {}\n",
    "    i = 1\n",
    "\n",
    "    for symbol_id in X.symbol_id.sort_values().unique():\n",
    "        print(f'Processing symbol_id: {symbol_id} ({i}/{len(X.symbol_id.unique())})')\n",
    "        print()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        # Get date of 75% of the data for this symbol\n",
    "        X.index = pd.to_datetime(X.index)\n",
    "        date_75 = X[X['symbol_id'] == symbol_id].index.to_series().quantile(0.75)\n",
    "\n",
    "        # Split the data into training and test set\n",
    "        X_train = X[(X.index <= date_75)]\n",
    "        X_test = X[(X['symbol_id'] == symbol_id) & (X.index > date_75)]\n",
    "\n",
    "        # Filter out rows where the price_close_rz_48 is less than 2\n",
    "        X_train = X_train[X_train['price_close_rz_48'] >= 2]\n",
    "        X_test = X_test[X_test['price_close_rz_48'] >= 2]\n",
    "\n",
    "        y_train = X_train['triple_barrier_label_w336_h336']\n",
    "        y_test = X_test['triple_barrier_label_w336_h336']\n",
    "\n",
    "        X_train = X_train.drop(columns = cols_to_drop)\n",
    "        X_test = X_test.drop(columns = cols_to_drop)\n",
    "\n",
    "        # Print distribution of the target variable\n",
    "        print('Y train distribution:')\n",
    "        print(y_train.value_counts(normalize = True))\n",
    "        print()\n",
    "        print('Y test distribution:')\n",
    "        print(y_test.value_counts(normalize = True))\n",
    "        print()\n",
    "\n",
    "        rf = RandomForestClassifier(\n",
    "            max_features = 0.2,\n",
    "            bootstrap = True,\n",
    "            random_state = 9 + 10,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        rf.fit(X_train, y_train, sample_weight = X_train['returns_1'].abs().values)\n",
    "\n",
    "        # Classification reports for the test set\n",
    "        y_pred_test_rf = rf.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred_test_rf, output_dict = True)\n",
    "\n",
    "        # Distribution of predictions\n",
    "        print('Predictions distribution:')\n",
    "        print(pd.Series(y_pred_test_rf).value_counts())\n",
    "        print()\n",
    "\n",
    "        print('RF Test:')\n",
    "        print(classification_report(y_test, y_pred_test_rf))\n",
    "        print()\n",
    "\n",
    "        # Difference between precision for the positive class and the proportion of positive class in the test set\n",
    "        # If the difference is positive, the model is better than the baseline of predicting the positive class\n",
    "        precision_diff = report['1.0']['precision'] - y_test.value_counts(normalize = True)[1]\n",
    "\n",
    "        performance_dict[symbol_id] = {\n",
    "            'precision': report['1.0']['precision'],\n",
    "            'recall': report['1.0']['recall'],\n",
    "            'f1-score': report['1.0']['f1-score'],\n",
    "            'precision_diff': precision_diff\n",
    "        }\n",
    "\n",
    "    return performance_dict\n",
    "\n",
    "performance_dict = get_model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert performance_dict to DataFrame\n",
    "performance_df = pd.DataFrame(performance_dict).T\n",
    "performance_df.index.name = 'symbol_id'\n",
    "performance_df = performance_df.sort_values('precision_diff', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.precision_diff.sort_values().plot(kind = 'barh', figsize = (20, 15))\n",
    "plt.title('Percentage Improvement Over Baseline Model for Predicting Profitable Trades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap performance_df to estimate the sampling distributions of the performance metrics\n",
    "n_bootstraps = 100_000\n",
    "\n",
    "bootstrap_precision_diff = []\n",
    "bootstrap_precision = []\n",
    "bootstrap_recall = []\n",
    "bootstrap_f1_score = []\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    bootstrap_precision_diff.append(performance_df['precision_diff'].sample(frac = 1, replace = True).mean())\n",
    "    bootstrap_precision.append(performance_df['precision'].sample(frac = 1, replace = True).mean())\n",
    "    bootstrap_recall.append(performance_df['recall'].sample(frac = 1, replace = True).mean())\n",
    "    bootstrap_f1_score.append(performance_df['f1-score'].sample(frac = 1, replace = True).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_bootstrapped_model_performance():\n",
    "    # 2 x 2 subplot for the bootstrapped distributions\n",
    "    plt.figure(figsize = (14, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot(bootstrap_precision_diff, bins = 100, kde = True)\n",
    "    plt.title(f'Distribution of {n_bootstraps:,} Bootstrapped Avg. OOS Precision Differences')\n",
    "    plt.xlabel('Precision Difference')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Scatter plot of the mean precision difference (one point)\n",
    "    plt.scatter(performance_df['precision_diff'].mean(), 50, color = 'r', s = 100, label = 'Mean Precision Difference')\n",
    "\n",
    "    # Plot the 95% confidence interval\n",
    "    plt.axvline(np.percentile(bootstrap_precision_diff, 2.5), color = 'r', linestyle = '--', label = '2.5th percentile')\n",
    "    plt.axvline(np.percentile(bootstrap_precision_diff, 97.5), color = 'r', linestyle = '--', label = '97.5th percentile')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(bootstrap_precision, bins = 100, kde = True)\n",
    "    plt.title(f'Distribution of {n_bootstraps:,} Bootstrapped Avg. OOS Precisions')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Scatter plot of the mean precision (one point)\n",
    "    plt.scatter(performance_df['precision'].mean(), 50, color = 'r', s = 100, label = 'Mean Precision')\n",
    "\n",
    "    # Plot the 95% confidence interval\n",
    "    plt.axvline(np.percentile(bootstrap_precision, 2.5), color = 'r', linestyle = '--', label = '2.5th percentile')\n",
    "    plt.axvline(np.percentile(bootstrap_precision, 97.5), color = 'r', linestyle = '--', label = '97.5th percentile')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(bootstrap_recall, bins = 100, kde = True)\n",
    "    plt.title(f'Distribution of {n_bootstraps:,} Bootstrapped Avg. OOS Recalls')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Scatter plot of the mean recall (one point)\n",
    "    plt.scatter(performance_df['recall'].mean(), 50, color = 'r', s = 100, label = 'Mean Recall')\n",
    "\n",
    "    # Plot the 95% confidence interval\n",
    "    plt.axvline(np.percentile(bootstrap_recall, 2.5), color = 'r', linestyle = '--', label = '2.5th percentile')\n",
    "    plt.axvline(np.percentile(bootstrap_recall, 97.5), color = 'r', linestyle = '--', label = '97.5th percentile')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.histplot(bootstrap_f1_score, bins = 100, kde = True)\n",
    "    plt.title(f'Distribution of {n_bootstraps:,} Bootstrapped Avg. OOS F1-Scores')\n",
    "    plt.xlabel('F1-Score')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Scatter plot of the mean f1-score (one point)\n",
    "    plt.scatter(performance_df['f1-score'].mean(), 50, color = 'r', s = 100, label = 'Mean F1-Score')\n",
    "\n",
    "    # Plot the 95% confidence interval\n",
    "    plt.axvline(np.percentile(bootstrap_f1_score, 2.5), color = 'r', linestyle = '--', label = '2.5th percentile')\n",
    "    plt.axvline(np.percentile(bootstrap_f1_score, 97.5), color = 'r', linestyle = '--', label = '97.5th percentile')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_bootstrapped_model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('95% Confidence Interval for Precision Difference:')\n",
    "print(np.percentile(bootstrap_precision_diff, [2.5, 97.5]))\n",
    "print()\n",
    "print('Average Precision Difference:')\n",
    "print(np.mean(bootstrap_precision_diff))\n",
    "print()\n",
    "print('95% Confidence Interval for Precision:')\n",
    "print(np.percentile(bootstrap_precision, [2.5, 97.5]))\n",
    "print()\n",
    "print('Average Precision:')\n",
    "print(np.mean(bootstrap_precision))\n",
    "print()\n",
    "print('95% Confidence Interval for Recall:')\n",
    "print(np.percentile(bootstrap_recall, [2.5, 97.5]))\n",
    "print()\n",
    "print('Average Recall:')\n",
    "print(np.mean(bootstrap_recall))\n",
    "print()\n",
    "print('95% Confidence Interval for F1-Score:')\n",
    "print(np.percentile(bootstrap_f1_score, [2.5, 97.5]))\n",
    "print()\n",
    "print('Average F1-Score:')\n",
    "print(np.mean(bootstrap_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
